{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Encoder-Decoder Model\n",
    "    - 1. Create anoter-side of Model by using current-side of Model\n",
    "    when it has any two-targets.\n",
    "    \n",
    "    - 2. For example, when we have English and Franch sentences,\n",
    "    we can create the Model that translate English to Franch.\n",
    "    \n",
    "    - 3. We might be can create Auto-Question-Answer when we have\n",
    "    two-targets, Question and Answer.\n",
    "    \n",
    "    - 4. Also, we can create explanation by combining-CNN-Images,\n",
    "    which mean this model is one of the important model while we\n",
    "    use DL(Deep-Learning)-Model and NN(Neural-Network)-Model.\n",
    "    \n",
    "    - 5. From here, we are going to create a Translation-Model that\n",
    "    translates English to Spanish.\n",
    "    \n",
    "    - 6. Planning to convert this code to be create-file, \n",
    "    the new README.md files which contains comments.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "remove_marks_regex = re.compile(\n",
    "    \"[\\,\\(\\)\\[\\]\\*:;¿¡]|<.*?>\"\n",
    ")\n",
    "shift_marks_regex = re.compile(\"([?!\\.])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = 0\n",
    "sos = 1\n",
    "eos = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    - 1.Delete Unnecessary-letter(Charcter) \n",
    "    after changed to Lower-case all, and then\n",
    "    Divide '!?' and word.\n",
    "    \n",
    "    - 2. Especially Spanish has the '¿' or '¡' \n",
    "    when they use exclamation and interrogative-Sentences,\n",
    "    But to Simplificate Symbol, I just left some Symbol that follows\n",
    "    English, and Delete all of Spanish Symbol\n",
    "    such as '¿' and '¡'. \n",
    "\"\"\"\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove Unnessary-Character(Letter).\n",
    "    text = remove_marks_regex.sub(\"\", text)\n",
    "    \n",
    "    # Insert blanks between '?!' and word.\n",
    "    text = shift_marks_regex.sub(r\" \\1\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    - 1. Convert 'spa.txt' of first-line to English and Spanish\n",
    "    as each of Token-List.\n",
    "    \n",
    "    - 2. 'build_vocab' is stands of for create Vocabulary.\n",
    "    \n",
    "    - 3. To Execute this function, makes sure you have to apply tags,\n",
    "    named Padding and Start, End.\n",
    "\"\"\"\n",
    "def parse_line(line):\n",
    "    line = normalize(line.strip())\n",
    "    \n",
    "    # Create each of Token which included Translation-Source(src)\n",
    "    # and Translation-Target(trg) as a List.\n",
    "    src, trg = line.split(\"\\t\")\n",
    "    src_tokens = src.strip().split()\n",
    "    trg_tokens = src.strip().split()\n",
    "    return src_tokens, trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokens):\n",
    "    # Check the count(Number) of appearence of token's \n",
    "    # in the All of sentences inside files.\n",
    "    counts = collections.Counter(tokens)\n",
    "    \n",
    "    # Place them in order to the count(Number) of appearence of token's\n",
    "    # from the oldest(highest).\n",
    "    sorted_counts = sorted(\n",
    "        counts.item(),\n",
    "        key=lambda c: c[1], reverse=True\n",
    "    )\n",
    "    \n",
    "    # Create Reverse-Directory and Forwarding-List Vocabulary\n",
    "    # by adding three-tags.\n",
    "    word_list = [\"<UNK>\", \"<SOS>\", \"<EOS>\"] \\\n",
    "        + [x[0] for x in sorted_counts]\n",
    "    word_dict = dict((w, i) for i, w in enumerate(word_list))\n",
    "    return word_list, word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    - 1. Convert Word-List as a Tensor.\n",
    "    \n",
    "    - 2. Define Max-Length and Pad(Padding) that are lacking.\n",
    "\"\"\"\n",
    "def words_to_tensor(words, word_dict, max_len, padding=0):\n",
    "    # Attach Finish(Latest)-Tag to the End.\n",
    "    words = words + [\"<EOS>\"]\n",
    "    \n",
    "    # Convert Numberic-List by applying 'Dictionary'.\n",
    "    words = [word_dict.get(w, 0) for w in words]\n",
    "    seq_len = len(words)\n",
    "    \n",
    "    # Padding(Pad) the Length if its or under the 'max_len'.\n",
    "    if seq_len < max_len + 1:\n",
    "        words = words + [padding] * (max_len + 1 - seq_len)\n",
    "    \n",
    "    # Return by convert Tensor.\n",
    "    return torch.tensor(words, dtype=torch.int64), seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationPairDataset(Dataset):\n",
    "    def __init__(self, path, max_len=15):\n",
    "        \n",
    "        # A function that filter-out the several-function \n",
    "        # which haves lots of length of word.\n",
    "        def filter_pair(p):\n",
    "            return not (len(p[0]) > max_len\n",
    "                       or len(p[1]) > max_len)\n",
    "        \n",
    "        # Filtering and Parse after open a file.\n",
    "        with open(path) as fp:\n",
    "            pairs = map(parse_line, fp)\n",
    "            pairs = filter(filter_pair, pairs)\n",
    "            pairs = list(pairs)\n",
    "            \n",
    "        # Device the Sentence as Source and Target.\n",
    "        src = [p[0] for p in pairs]\n",
    "        trg = [p[1] for p in pairs]\n",
    "        \n",
    "        # Create each of Vocabulary\n",
    "        self.src_word_list, self.src_word_dict = \\\n",
    "            build_vocab(itertools.chain.from_iterable(src))\n",
    "        self.trg_word_list, self.trg_word_dict = \\\n",
    "            build_vocab(itertools.chain.from_iterable(trg))\n",
    "        \n",
    "        # Convert as Tensor by applying Vocabulary.\n",
    "        self.src_data = [words_to_tensor(\n",
    "            words, self.src_word_dict, max_len)\n",
    "                         for words in src\n",
    "        ]\n",
    "        self.trg_data = [words_to_tensor(\n",
    "            words, self.trg_word_dict, max_len, -100)\n",
    "                         for words in trg\n",
    "        ]\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.src_data)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            src, lsrc = self.src_data[idx]\n",
    "            trg, ltrg = self.trg_data[idx]\n",
    "            return src, lsrc, trg, ltrg\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_len = 10\n",
    "path = \"../05/spa.txt\"\n",
    "# ds = TranslationPairDataset(path, max_len=max_len)\n",
    "# loader = DataLoader(ds, batch_size=batch_size, shuffle=True,\n",
    "#                    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
